
# DÃ©tection de fraudes bancaires en temps rÃ©el

Ce projet a Ã©tÃ© rÃ©alisÃ© dans le cadre de la formation Data Analyst Ã  la Wild Code School. Il simule un flux de transactions bancaires, les analyse via un modÃ¨le de Machine Learning (XGBoost) et monitore les performances en temps rÃ©el.

## ğŸ“º DÃ©monstration vidÃ©o

<p align="center">
  <a href="https://www.youtube.com/watch?v=ouKKv2ohZn0">
    <img src="./images/image_youtube.png" alt="DÃ©monstration VidÃ©o" width="600">
  </a>
</p>

*Cliquez sur l'image pour voir la dÃ©monstration (2:44)*

## L'Ã‰quipe
* **FrÃ©dÃ©ric Bayen** - *Architecture MLOps, BigQuery, Streamlit, FastAPI & Automatisation*
* **Kenji Victor** - *Streamlit, Grafana & Prometheus, FastAPI*
* **Jean-Baptiste Leduc** - *Data Visualization, Streamlit Dashboards, Redis & ModÃ©lisation XGBoost*

## Vision Business & storytelling

### Le constat : Une hÃ©morragie financiÃ¨re

Imaginez un Ã©cosystÃ¨me de Mobile Money sâ€™appuyant sur un rÃ©seau de milliers d'agents de proximitÃ©. Chaque jour, des millions d'utilisateurs transforment leur tÃ©lÃ©phone en portefeuille pour des transactions vitales : commerce, aide aux proches ou Ã©pargne.

Pourtant, cette infrastructure de confiance est devenue la cible privilÃ©giÃ©e de rÃ©seaux criminels experts en ingÃ©nierie sociale et en fraude transactionnelle. Pour notre institution, cette vulnÃ©rabilitÃ© ne se limite pas Ã  un risque numÃ©rique ; elle se traduit par des pertes rÃ©elles s'Ã©levant Ã  **plusieurs centaines de millions d'euros par an.**


### La problÃ©matique : L'Ã©quilibre entre sÃ©curitÃ© et fluiditÃ©

Le dÃ©fi posÃ© Ã  notre Ã©quipe : stopper la fraude sans dÃ©grader l'expÃ©rience utilisateur.

 - **RapiditÃ©** : La dÃ©cision (bloquer ou autoriser) doit Ãªtre rendue en quelques millisecondes pour ne pas ralentir le client.

 - **Satisfaction Client** : Un "Faux Positif" (client honnÃªte bloquÃ© par erreur) est commercialement problÃ©matique et a un coÃ»t financier non nÃ©gligeable.


### La solution : Un systÃ¨me vivant et auto-adaptatif

PlutÃ´t qu'un modÃ¨le statique, nous avons conÃ§u une infrastructure Ã©volutive. GrÃ¢ce Ã  notre pipeline MLOps, le systÃ¨me apprend en continu. DÃ¨s que de nouvelles typologies de fraude apparaissent, le modÃ¨le se rÃ©entraÃ®ne automatiquement pour s'adapter aux nouvelles menaces, garantissant une protection toujours Ã  jour.


### Le pilotage

Pour garder un contrÃ´le total sur la solution, nous avons dÃ©ployÃ© deux centres de commandement :

 - Pour garder le contrÃ´le, nous avons dÃ©veloppÃ© un **panneau de suivi Streamlit**. Il permet de visualiser les flux en temps rÃ©el, d'analyser les comportements suspects et de piloter la stratÃ©gie de sÃ©curitÃ© de la banque. C'est ici que l'intelligence artificielle rencontre l'humain.

![DÃ©tection de fraudes](images/streamlit_fraude.gif)

 - **La supervision infrastructure (Grafana & Prometheus)** : Cette interface surveille la santÃ© technique du systÃ¨me. Nous suivons en temps rÃ©el la consommation CPU/RAM de chaque conteneur et la latence de l'API pour garantir une haute disponibilitÃ© et des performances constantes sous la charge.

 ![Monitoring Grafana](images/grafanaa.gif)


### Nos rÃ©sultats sur la fraude

1. **Le Bouclier (Recall de 87 %)** : Nous interceptons dÃ©sormais la grande majoritÃ© des tentatives de fraude.

2. **La fluiditÃ© client (SpÃ©cificitÃ© de 99,4 %)** : Nous garantissons une expÃ©rience sans problÃ¨me. 99,4 % des transactions lÃ©gitimes sont validÃ©es instantanÃ©ment, minimisant ainsi le mÃ©contentement client.

3. **L'efficacitÃ© des alertes (PrÃ©cision de 63 %)** : Sur l'ensemble des transactions bloquÃ©es pour suspicion, prÃ¨s de 2 sur 3 sont rÃ©ellement des fraudes. Ce score Ã©levÃ© permet aux Ã©quipes de sÃ©curitÃ© de se concentrer sur des menaces hautement probables plutÃ´t que de traiter un volume ingÃ©rable de fausses alertes.

![MÃ©triques du modÃ¨le](images/performances_modele.gif)


### Note sur la simulation de la "VÃ©ritÃ© Terrain"

Dans ce projet, les transactions envoyÃ©es vers BigQuery incluent la valeur rÃ©elle de fraude.

Pourquoi ce choix ? Dans un environnement bancaire rÃ©el, il existe un dÃ©calage temporel : le modÃ¨le prÃ©dit une fraude Ã  l'instant T, et la confirmation rÃ©elle (le "retour client" ou le signalement) arrive plus tard.

Pour les besoins de la dÃ©monstration en temps rÃ©el et pour permettre au cycle d'auto-apprentissage (MLOps) de fonctionner de maniÃ¨re fluide, nous avons "compressÃ© le temps". Nous simulons ce retour d'information instantanÃ©ment afin de dÃ©montrer la capacitÃ© du pipeline Ã  :

 - DÃ©tecter l'apparition de nouveaux patterns.

 - DÃ©clencher un rÃ©entraÃ®nement automatique basÃ© sur des donnÃ©es vÃ©rifiÃ©es.

- Comparer immÃ©diatement la prÃ©diction du modÃ¨le avec la rÃ©alitÃ© pour calculer les mÃ©triques de performance.


---


## Architecture du Pipeline

L'application repose sur une architecture micro-services conteneurisÃ©e avec Docker.

**La stack technique**

![Stack Technique](images/resume_stack.png)

**Justification des choix techniques**

Le choix de cette stack repose sur trois impÃ©ratifs : la vitesse de dÃ©tection (temps rÃ©el), la fiabilitÃ© des donnÃ©es et l'automatisation du cycle de vie du modÃ¨le.

 **-> Ingestion & rÃ©silience (Le flux de donnÃ©es)**

 - **FastAPI :** Choisi pour ses performances asynchrones natives, permettant de traiter des milliers de requÃªtes de transactions par seconde avec une latence minimale.

 - **Redis (Buffer d'ingestion) :** Il joue un rÃ´le de tampon critique. En stockant temporairement les transactions entrantes avant leur envoi vers BigQuery, il protÃ¨ge l'API des variations de latence du rÃ©seau. Cela garantit qu'aucune donnÃ©e de transaction n'est perdue, mÃªme en cas de pic de trafic ou de ralentissement momentanÃ© des services Cloud.

- **Google BigQuery**: C'est ici que l'historique complet est archivÃ© de maniÃ¨re structurÃ©e pour permettre un rÃ©entraÃ®nement prÃ©cis du modÃ¨le sur des volumes massifs.

**-> Intelligence & automatisation**

- **XGBoost :** SÃ©lectionnÃ© pour sa gestion efficace des donnÃ©es tabulaires et sa capacitÃ© Ã  traiter les valeurs manquantes ou les distributions complexes, surpassant les modÃ¨les de deep learning classiques sur ce type de donnÃ©es de fraude. Il permet aussi l'utilisation de CUDA

- **Prefect :** Pilote le cycle de vie complet du ML (rÃ©cupÃ©ration BigQuery, gestion des Ã©checs, dÃ©ploiement). Nous l'avons prÃ©fÃ©rÃ© Ã  Airflow car il est beaucoup moins lourd, plus flexible et permet une orchestration "Python-first" sans la complexitÃ© de gestion d'infrastructure d'un serveur Airflow complet.

**-> ObservabilitÃ© & interface**

 - **Prometheus & Grafana :** Assurent le monitoring technique de l'ensemble de l'infrastructure. Prometheus collecte les mÃ©triques brutes de chaque conteneur (usage CPU, consommation RAM, latence rÃ©seau), tandis que Grafana les transforme en tableaux de bord visuels. Cela permet de surveiller en temps rÃ©el les ressources utilisÃ©es par le pipeline.

 - **Streamlit :** Offre une interface utilisateur fluide pour visualiser les rapports de fraude et les rÃ©sultats du modÃ¨le de maniÃ¨re intuitive.

 - **Docker :** Encapsule chaque brique de cette stack pour garantir une portabilitÃ© totale en local.


---

**Le pipeline**


```text
[ SOURCE : PaySim_stream.csv ]
+-------------------------------+
| Transactions (streamenvoi.py) |
+-------------------------------+
      |
 FAST |
 API  | 
      v
[ CERVEAU : API + ModÃ¨le ] 
+-----------------------+       +-------------------+            
|  streamrecepteur.py   | ----> |  ML_XGBoost.ipynb |             
|  (FastAPI + modÃ¨le)   | <---- |  ModÃ¨le XGBoost   |<------------+   
+-----------------------+       +-------------------+             |
      âˆ§               âˆ§                                           |
      |               | RÃ©sultats (RESP)                          |
      |               v                                           |
      |           [ STOCKAGE : Redis ]                            |
      |     +------------------------------------------+    [ MLOPS : Prefect ]
      |     |              REDIS (Cache)               |    +-----------------+
      |     |  - flux_global (Archive BigQuery)        |    |  retrain.py     |
 FAST |     |  - flux_streamlit (Affichage direct)     |    |  (Auto-Train)   |
 API  |     +------------------------------------------+    +-----------------+
      |                     |                                     âˆ§ 
      |                     | Archivage (RESP)                    | API BigQuery et SQL
      |                     v                                     v 
      |                +-------------------+                [ Data Warehouse ]
      |                |   worker_bq.py    |  API BigQuery  +-----------------+
      |                | (Envoi BigQuery)  |--------------->|      BigQuery   |
      |  Monitoring    +-------------------+                +-----------------+
      v                                                     
      +----------------------------------------------------------+
      |                                                          |     
      v                                                          v
[ SUPERVISION : Prometheus & Grafana ]             [ TABLEAU DE BORD : Streamlit]
+------------------------------------------+    +------------------------------------------+
| - Metrics systÃ¨me (CPU/RAM conteneurs)   |    | dashboard.py                             |
| - Metrics business (Taux de fraude)      |    | - Dashboarding & Alerting Temps RÃ©el     |
| - Metrics FASTAPI                        |    | - EDA                                    |
+------------------------------------------+    +------------------------------------------+
```

---

## Gestion des donnÃ©es

Le projet utilise le dataset PaySim [(disponible ici sur Kaggle)](https://www.kaggle.com/datasets/ealaxi/paysim1/data).

Pour simuler un environnement de production rÃ©el, nous avons crÃ©Ã© un script ```decoupe.py``` pour segmenter les donnÃ©es :

 - **90% (Historique)** : UtilisÃ©es pour l'entraÃ®nement initial et stockÃ©es comme base de rÃ©fÃ©rence.

 - **10% (Flux Stream)** : IsolÃ©es pour simuler l'envoi de transactions ligne par ligne par ```streamenvoi.py```.

Cette mÃ©thode garantit que le modÃ¨le est testÃ© sur des donnÃ©es qu'il n'a jamais rencontrÃ©es lors de sa phase d'apprentissage initiale.

---

## Lancement rapide

### PrÃ©requis

   - **Docker** & **Docker Compose** installÃ©s.

   - **BigQuery** : CrÃ©er un projet sur la Console Google Cloud.




### Installation

1. **Cloner le projet.**

2. **TÃ©lÃ©charger le dataset PaySim.csv [(disponible ici sur Kaggle)](https://www.kaggle.com/datasets/ealaxi/paysim1/data).** et le placer dans ```./data/```


3. **Copier le template des variables d'environnement**

      ```cp .env-dist .env```

      -> Remplir le .env avec vos variables d'environnement

4. **Copier le template des variables de rÃ©-entraÃ®nement du modÃ¨le**

      ```cp state.json-dist state.json```

      -> Remplir le state.json avec vos valeurs

5. **Initialisation du projet (uv)**

      ```uv sync```

6. **DÃ©coupe du dataset**

      ```uv run notebooks/decoupe.py```
   
      -> Dataset ```PaySim_stream.csv``` et ```PaySim_historical.csv``` crÃ©Ã©s dans le dossier `./data/`

7. **GÃ©nÃ©rer la clÃ© Json Bigquery**

      - Activer l'API BigQuery, et crÃ©er un compte de service avec les rÃ´les BigQuery Admin et Storage Admin.

      - GÃ©nÃ©rer une clÃ© JSON, la nommer ```gcp-key.json``` et la placer Ã  la racine du projet.

8. **Ingestion des donnÃ©es historiques dans BigQuery**

      ```uv run src/ingestion/ingestion.py```

9. **Lancer l'infrastructure :**

      ```docker compose up --build```

---

### AccÃ¨s aux services

 - **Dashboard Streamlit** : http://localhost:8501

 - **Documentation API** : http://localhost:8000/docs / http://localhost:8000/report / http://localhost:8000/reload

 - **Monitoring Grafana** : http://localhost:3000

 - **Prometheus** : http://localhost:9090

 - **Processus de rÃ©entrainement (Prefect)** : http://localhost:4200/dashboard

 - **Processus d'envoi vers BigQuery** : ```docker logs -f worker-bigquery```

 - **Redis** : lister les listes : ```KEYS *``` / afficher l'intÃ©gralitÃ© d'une liste : ```LRANGE liste_fraudes 0 -1```

---

## Performance du modÃ¨le (XGBoost)

Compte tenu du fort dÃ©sÃ©quilibre des donnÃ©es (99,87% de transactions saines vs 0,13 % de fraudes), l'Accuracy (prÃ©cision globale) n'est pas un indicateur pertinent. Nous nous concentrons sur la capacitÃ© du modÃ¨le Ã  dÃ©tecter les fraudes rÃ©elles.

**RÃ©sultats sur le Test Set de la v1**

MÃ©trique|Valeur|InterprÃ©tation
| :--- | :--- | :--- |
| **Recall** | 87 % | Le modÃ¨le identifie avec succÃ¨s 87 % des tentatives de fraude. | 
|**PrÃ©cision** | 63 % | Lorsqu'on prÃ©dit une fraude, elle est rÃ©elle dans 63 % des cas. | 
|**F1-Score** | 73 % | Un excellent Ã©quilibre pour un systÃ¨me de dÃ©tection en temps rÃ©el. | 

---

## Automatisation MLOps

Le conteneur `retrain-automation` surveille la table BigQuery via Prefect.

 - ModularitÃ© : Le seuil de dÃ©clenchement (```min_rows_to_retrain```), le nombre de transactions rÃ©cupÃ©rÃ©es sur BigQuery  (```limit_sql```) et l'intervalle de vÃ©rification (```check_interval_secondes```) sont modifiables sans redÃ©marrage dans ```state.json```.

 - Action : DÃ¨s que le seuil est atteint, le modÃ¨le est rÃ©entraÃ®nÃ© sur les nouvelles donnÃ©es, archivÃ©, et l'API est notifiÃ©e pour charger la nouvelle version instantanÃ©ment.

![Automatisation du rÃ©entrainement](images/prefect2.gif)

### Validation et sÃ©curitÃ© du pipeline (Model Gating)

Pour garantir que la qualitÃ© du service ne se dÃ©grade jamais, nous avons implÃ©mentÃ© une sÃ©curitÃ© de type **"Model Gating"**. Le systÃ¨me compare systÃ©matiquement le `Recall` du nouveau modÃ¨le entraÃ®nÃ© avec le record historique.

**Fonctionnement observÃ© :**

| Ã‰tape | Logique appliquÃ©e |
| :--- | :--- |
| **Phase d'apprentissage** | Lors de l'initialisation (ou aprÃ¨s un reset), le systÃ¨me enregistre les premiÃ¨res performances comme nouveaux records Ã  battre. |
| **SÃ©curitÃ© active** | Si un rÃ©entraÃ®nement produit un modÃ¨le dont le Recall est infÃ©rieur au record (ex: 91.69% vs 93.49%), le pipeline **rejette** automatiquement la mise Ã  jour et conserve l'ancien "champion" en production. |

> **Note technique :** Ce mÃ©canisme protÃ¨ge contre les rÃ©entraÃ®nements sur des donnÃ©es bruitÃ©es ou des rÃ©gressions de performance. L'API n'est notifiÃ©e (`/reload`) que si le statut passe Ã  "Mis en production".

![Statut](images/statut_modele.png)

---

## Structure du dossier

```
â”œâ”€â”€ data/                  # Datasets (CSV historiques et flux stream)
â”œâ”€â”€ grafana/               # Configuration du monitoring
â”‚   â”œâ”€â”€ dashboards/        # Fichiers JSON des dashboards (RAM, Principal, etc.)
â”‚   â””â”€â”€ provisioning/      # Configuration automatique des sources de donnÃ©es
â”œâ”€â”€ notebooks/             # Travail exploratoire et recherche
â”‚   â”œâ”€â”€ decoupe.py         # Script de split du dataset (90/10)
â”‚   â”œâ”€â”€ EDA_PaySim.ipynb   # Analyse exploratoire des donnÃ©es
â”‚   â””â”€â”€ ML_XGBoost.ipynb   # EntraÃ®nement et tests du modÃ¨le
â”œâ”€â”€ src/                   # Code source applicatif
â”‚   â”œâ”€â”€ API/               # streamrecepteur (FastAPI), streamenvoi et worker_bq
â”‚   â”œâ”€â”€ dashboard/         # Interface utilisateur Streamlit (dashboard.py)
â”‚   â”œâ”€â”€ ingestion/         # Scripts de traitement des donnÃ©es
â”‚   â”œâ”€â”€ models/            # Fichiers .joblib (pipeline_latest, archives)
â”‚   â””â”€â”€ retrain/           # Automatisation MLOps (retrain.py)
â”œâ”€â”€ docker-compose.yml     # Orchestration des services Docker
â”œâ”€â”€ Dockerfile             # Configuration de l'image Python/UV
â”œâ”€â”€ prometheus.yml         # Configuration de la collecte des mÃ©triques
â”œâ”€â”€ state.json             # Ã‰tat dynamique et configuration du rÃ©entraÃ®nement
â””â”€â”€ README.md              # Documentation du projet
```

---

## Maintenance et rÃ©initialisation

Pour remettre le projet Ã  zÃ©ro, tapez uniquement cette CLI  : ```python reset_projet.py```

 - **ModÃ¨les** : Suppression de `pipeline_latest.joblib` et vidage des archives.

 - **Ã‰tat** : RÃ©initialisation du compteur `last_count` dans `state.json`.

 - **Infrastructure** : Purge totale de Redis (via Docker) et des tables BigQuery.

---

## ProblÃ¨mes rencontrÃ©s & solutions apportÃ©es


| DÃ©fi Technique | Impact | Solution apportÃ©e |
| :--- | :--- | :--- |
| **DÃ©sÃ©quilibre des classes** | Dataset Ã  0.13% de fraudes, biaisant fortement les prÃ©dictions initiales. | Utilisation de `scale_pos_weight` calculÃ© dynamiquement sur le ratio rÃ©el Fraude/Normal lors du rÃ©entraÃ®nement. |
| **Performance de l'entraÃ®nement** | RandomForest trop lent pour l'optimisation par GridSearch (estimÃ© Ã  plusieurs mois). | Passage Ã  **XGBoost (CUDA/GPU)** et utilisation de **RandomizedSearch** pour une optimisation rapide. |
| **Optimisation du rÃ©entraÃ®nement** | Temps de calcul excessif et risques d'incompatibilitÃ© matÃ©rielle **(CUDA/GPU)** selon l'hÃ´te.| Mise en place d'un **Ã©chantillonnage intelligent** : filtrage des donnÃ©es BigQuery pour entraÃ®ner sur un volume optimal, garantissant un cycle MLOps rapide et compatible CPU|
| **Arbitrage technologique du Front-end** | Risque de dÃ©passement des dÃ©lais dÃ» Ã  la courbe d'apprentissage Ã©levÃ©e de Django pour l'interface de monitoring. | Pivot vers **Streamlit** : dÃ©veloppement rapide d'un dashboard interactif "Python-native" parfaitement adaptÃ© aux besoins data. |
| **Affichage Temps RÃ©el** | Interface Streamlit statique par dÃ©faut, ne reflÃ©tant pas le flux entrant. | Boucle `while True` avec placeholders `st.empty()` pour rafraÃ®chir les KPIs sans rechargement de page. |
| **Reset de Redis** | La consommation des donnÃ©es par le worker BigQuery vidait le cache Redis, rendant les donnÃ©es indisponibles pour le dashboard. | Mise en place d'un **double flux** : un flux persistant pour l'UI Streamlit et un autre pour l'archivage BigQuery. |
| **Choix de l'Orchestrateur** | Airflow s'est rÃ©vÃ©lÃ© trop complexe et gourmand en ressources pour ce projet. | Pivot vers **Prefect**, plus lÃ©ger, moderne et parfaitement adaptÃ© Ã  notre architecture Ã©vÃ©nementielle. |
| **Apprentissage Docker** | ComplexitÃ© des rÃ©seaux inter-conteneurs et des dÃ©pendances pour des novices. | Gestion des ordres de dÃ©marrage (`depends_on`) et isolation des rÃ©seaux internes (`networks`). |
| **Synchronisation du Pipeline** | Risque de charger un modÃ¨le incomplet pendant l'Ã©criture disque. | SystÃ¨me de **notification Push** : l'API recharge le modÃ¨le via `/reload` uniquement aprÃ¨s confirmation de sauvegarde complÃ¨te. |
| **Data Leakage** | Score de performance artificiellement Ã©levÃ© (99.9%) via les variables `newbalance`. | **Suppression prÃ©ventive** des variables "du futur" (`newbalanceOrig/Dest`). |
| **Persistance et portabilitÃ© du monitoring** | Perte systÃ©matique des visualisations Grafana lors du redÃ©marrage des conteneurs (non-persistance). | Automatisation via **Dashboard-as-Code** (Provisioning) : injection de fichiers JSON et YAML pour un dÃ©ploiement reproductible. |
| **Monitoring : rupture des liens Data Source** | Tableaux de bord vides aprÃ¨s import automatique car l'ID et l'adresse IP de la source de donnÃ©es changeaient. | Fixation des adresses IP et UID des containers via YAML et mise Ã  jour du schÃ©ma JSON pour garantir la connexion immÃ©diate au dÃ©ploiement. |
| **Exposition des mÃ©triques de fraude** | Conflit d'exÃ©cution entre le serveur de mÃ©triques Prometheus et la boucle d'Ã©vÃ©nement asynchrone de FastAPI. | IntÃ©gration de `prometheus-fastapi-instrumentator` pour exposer les mÃ©triques sur un endpoint `/metrics` unifiÃ©. |


